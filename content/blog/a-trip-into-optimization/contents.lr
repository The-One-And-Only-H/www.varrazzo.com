title: A trip into optimization
---
pub_date: 2020-05-17
---
author: Daniele Varrazzo
---
image: /img/blog/aisle.jpg
---
_discoverable: no
---
tags:

software
psycopg
development
---
body:

I have been cheerfully hacking on psycopg3 features for a while: reached the
symbolic but important milestone of `DBAPI 2.0 compatibility`__, and using the
psycopg2 guidelines to add features progressively.

.. __: https://github.com/psycopg/psycopg3/commit/be4e3ee85dc773878d67edf0b7880824bf7059aa


Then I thought about checking how it was doing about speed... it wasn't a happy
day.

A few years ago, a new PostgreSQL driver was released: `asyncpg`_. It is
designed to be *just fast*, bar nothing else. It parses the `PostgreSQL
frontend/backend protocol`__ using its own implementation (as opposite as
using the libpq_, the PostgreSQL client library). asyncpg is designed to be
streamlined within the flow of the line protocol rather to have a
developer-friendly or recognisable interface: it doesn't follow a DBAPI_
design, it uses PostgreSQL native placeholders for queries, i.e. ``where
someting = $1``, no positional or named ones (the familiar ``%s`` and
``%(thing)s``). It is definitely a fast library, but like a Formula 1 not for
everybody to drive. Dropping it in your existing project means having to
rewrite all your queries and all the Python code interacting with the
database; no Django, no SQLAlchemy, and of course it's only asynchronous, so
not the natural choice if your program uses a different concurrency model or
needs none at all.

.. _asyncpg: https://magicstack.github.io/asyncpg/
.. __: https://www.postgresql.org/docs/current/protocol.html
.. _libpq: https://www.postgresql.org/docs/current/libpq.html
.. _DBAPI: https://www.python.org/dev/peps/pep-0249/

Anyway I thought to use `the toolbench they wrote`__ to compare the speed so
far against psycopg2 and asyncpg. How bad was my day?

.. __: https://github.com/magicstack/pgbench

..
    for d in asyncpg aiopg-tuples psycopg3-async; do \
        python pgbench_python.py $d -C1 ../queries/1-pg_type.json -D 10 --warmup-time 1 \
        --pghost localhost --pgport 5432 --pguser piro --output-format json \
        | jq -j "\"$d \" ,  .queries , \"\n\"" ; \
    done

================ =======
Driver           Queries
================ =======
asyncpg            15263
psycopg2 + aiopg    8871
psycopg3 async       426
================ =======

...this bad.

These are the number of queries run in 10 seconds. The numbers in themselves
are to be taken with a big pinch of salt: they are obtaining connecting the
driver to localhost so they are more CPU-bound than the environment a database
driver usually lives into, they are *one specific query* (the first in their
toolbench), nothing statistically sound... but still a pretty depressing
picture.

Back then the implementation of psycopg3 was pure Python, using ctypes to call
into the libpq shared library. The plan of writing an optimised C
implementation was already there, but this gave a measure of how badly needed
it is.


Profiling
=========

First step to climb such a mountain is to look at profiling information, which
can show function by function where time is spent.

.. figure:: profile-7b3b7f9.png
    :align: center
    :target: profile-7b3b7f9.svg

    Profile graph at commit ``7b3b7f9``

The graph is generated by `gprof2dot`_ using something like:

.. code:: bash

    python -m cProfile -o OUT.pstat script.py
    gprof2dot -f pstats OUT.pstat | dot -Tsvg > OUT.svg

.. _gprof2dot: https://github.com/jrfonseca/gprof2dot

..
    python -m cProfile -o psycopg3-async.pstat pgbench_python.py psycopg3-async \
        -C1 ../queries/1-pg_type.json -D 10 --warmup-time 1 \
        --pghost localhost --pgport 5432 --pguser piro --output-format json
    gprof2dot -f pstats psycopg3-async.pstat | dot -Tsvg > psycopg3-async-prof.svg

You can see more details clicking on the above graph. The left branch is
initialisation faff; the interesting part is on the right, following the path
of orange, yellow, green, showing the function callss where the program spent
most of its time. ``get_value`` is a pretty hot one, taking about the 50% of
the run time. It is called very often, once per every field of every record,
1.6M times in this graph. At that point it looked like:

.. code:: python

    def get_value(self, row_number: int, column_number: int -> Optional[bytes]:
        length = impl.PQgetlength(
            self.pgresult_ptr, row_number, column_number
        )
        if length:
            v = impl.PQgetvalue(self.pgresult_ptr, row_number, column_number)
            return string_at(v, length)
        else:
            if impl.PQgetisnull(self.pgresult_ptr, row_number, column_number):
                return None
            else:
                return b""

This function returns the content in bytes of a field. If it's 0-length it
checks if it's a NULL. The ``impl`` functions are ctypes calls to the libpq.
This made me think that the overhead of ctypes is not so tiny after all.


The cffi attempt
================

One thing I thought about doing was also to provide a cffi_ wrapper for the
libpq. cffi is a more popular choice than ctypes and PyPy can also make good
use of it. ctypes has the advantage of being packaged in the standard library
and not require an external dependency. However, unlike ctypes, cffi still
requires a compiler to create more efficient wrapper.

.. _cffi: https://cffi.readthedocs.io/

Porting the libpq wrapper to cffi was relatively straightforward and the
number of queries run was more than twice:

============== ======= =======
Implementation Commit  Queries
============== ======= =======
ctypes         7b3b7f9 426
cffi           dfe2893 902
============== ======= =======

Profiling__ showed that the time spent in get_value was down from the 50% to
the 35%, suggesting that cffi overhead is lower than ctypes. However
installing the cffi package already requires the presence of a C compiler, the
``libpq-dev`` package, and finding the dreaded ``pg_config`` executable: the
same requirements a C extension implementation would have, but with less
possibilities for optimization. For this reason probably cffi will remain a
side thing (possibly if the PyPy folks will be interested in it) and this code
is now parked `into a branch`__.

.. __: profile-cffi-dfe2893.svg
.. __: https://github.com/psycopg/psycopg3/tree/cffi


The Cython attempt
==================

The next and more reasonable way to step up performance is to create a C
extension. As highlighted in :ref:`blog/thinking-psycopg3` the idea was
already in place, allowing to choose between simple to install vs. fast. A
Python C extension can be written in pure C, as psycopg2 is, but there are
languages helping to handle some of the tedious aspects: the boilerplate
required to create objects and module and the reference count in particular.
The most current of such languages is Cython_, an evolution of Pyrex_, which I
had used a ridiculous number of years ago.

.. _Cython: https://cython.readthedocs.io/
.. _Pyrex: https://www.cosc.canterbury.ac.nz/greg.ewing/python/Pyrex/

The first step was a straightforward porting of the ``psycogp3.pq`` module:
that alone showed a promising class of results:

============== ======= =======
Implementation Commit  Queries
============== ======= =======
ctypes         7b3b7f9 426
cffi           dfe2893 902
Cython pq      6c6cbe3 1618
============== ======= =======


Speeding up adaptation
======================

Profiling__ now showed that the time spent in ``get_value()`` was now reduced
to a paltry 6% of the whole runtime: this means that that the hot path of the
program had moved to a different place.

.. __: profile-c-6c6cbe3.svg

The entire adaptation of the returned dataset takes the 60% which is a likely
next candidate, visible in lime/green. The ``<genexpr>`` box has a large
number of called functions: that function is the dispatcher choosing a
different adapter function for each PostgreSQL data type, and these were still
written in Python.

There is probably some value in creating analogous conversion functions in C,
but it wouldn't be the best way to optimise this job: the loader function have
all the same signature, for instance the loader for the binary representation
of a 32 bits integer is:

.. code:: python

    _int4_struct = struct.Struct("!i")

    def load_int4_binary(data: bytes) -> int:
        return _int4_struct.unpack(data)[0]

In order to feed data to this function it is necessary to:

- get the pointer and length to the returned data from the ``PGresult``
  struct (calls to ``PQgetvalue`` and ``PQgetlength``, which are lightweight);
- convert it to a Python ``bytes`` object, meaning a ``memcpy`` and a Python
  object allocation;
- calling the adaptation function (Python or C).

If we decide to not leave the C layer in the adaptation phase, then a better
signature can be used:

.. code:: pyrex

    cdef object load_int4_binary_c(const char *data, size_t length, void *context):
        return PyLong_FromLong(<int32_t>be32toh((<uint32_t *>data)[0]))

This allows reaching the loader functions making no extra copy of the data
received from the network and without creating intermediate Python object.
However, because the C functions are not accessible from Python, it opens the
question about how to customize the adapter, in case someone wanted to replace
a builtin one (a typical case is wanting to convert ``numeric`` into Python
``float`` instead of ``Decimal``). For the moment the mechanism works this
way:

- At import time the Python adapters are associated to data types OIDs; the
  current mechanism is through a decorator:

  .. code:: python

    # in psycopg3/types/numeric.py

    @Loader.binary(builtins["int4"].oid)
    def load_int4_binary(data: bytes) -> int:
        return _int4_struct.unpack(data)[0]

- If the C extension module is to be used, a C function exposed to Python is
  called, ``register_builtin_c_loaders()``.

  .. code:: python

    # in psycopg3/__init__.py

    # register default adapters
    from . import types

    # Override adapters with fast version if available
    if pq.__impl__ == "c":
        from ._psycopg3 import register_builtin_c_loaders
        register_builtin_c_loaders()

- This in turns calls a registration
  function for all the loaders for which there is a C implementation:

  .. code:: pyrex

    # in psycopg3/types/numeric.pyx

    from psycopg3.types import numeric
    register_c_loader(numeric.load_int4_binary, load_int4_binary_c)

- When the result of a query is returned to the client, the OIDs of the
  columns are used to select a Python loader. In Python the loaders are put
  into a list having as many items as the columns number; in Cython they are
  put into a C array of structures holding both a Python function and
  optionally a C function as members: If a Python loader has an optimized
  version registered, then that function is added to the array of loaders.

  ======  ===  ====================  ======================
  Type    OID  Python loader         C loader
  ======  ===  ====================  ======================
  text    25   ``load_text_binary``  ``load_text_binary_c``
  int4    23   ``load_int4_binary``  ``load_int4_binary_c``
  custom  ??   ``load_custom_type``  ``NULL``
  ======  ===  ====================  ======================

- When the data is converted to Python, for each column the corresponding
  loader is used:

  - if it has a C loader, the value and lengths are read from the result and
    passed to the function;

  - if it hasn't one, a ``bytes`` object is created and passed to the Python
    function.

This mechanism allows overriding the mapping from OID to types in Python and
to duplicate it in C. It has the disadvantage that if someone wanted to derive
an adapter for a custom data type (e.g. calling the text adapter and then
parsing the result) then the Python adapter would be used. There are probably
chances to improve this design, but it is a good starting point for the
moment.

It *might* be possible to avoid the ``memcpy`` by creating a memory view
instead of a bytes array, but I haven't quite figured out how to do it in
ctypes.


============== ======= =======
Implementation Commit  Queries
============== ======= =======
ctypes         7b3b7f9 426
cffi           dfe2893 902
Cython pq      6c6cbe3 1618
Cython adapt   b28b389 7511
============== ======= =======


Profiling against psycopg2
==========================

I was having concerns for a while that there was something stupidly wrong in
psycopg3 code, for instance in the way e.g. waiting was performed, or some
other silly mistake. asyncpg is too radically different to provide a direct
comparison, but a comparison with psycopg2 would have been interesting.
Unfortunately psycopg2, being pure C code, doesn't allow profiling with a
Python profiler (Cython explicitly provides hooks for that). But where a
tracing profiler cannot trace, a sampling profiler can sample: `py-spy`_ is
such a profiler. The principle is to interrupt a process several times per
second to look at its call stack to infer what it is doing: as long as enough
debug informations are compiled in, it is possible to obtain clear information
about C processes too. And of course, the most important property good software
should have: it produces pretty graphs:

.. _py-spy: https://github.com/benfred/py-spy

.. figure:: flame-psycopg3-b662c9d.png
    :align: center
    :target: flame-psycopg3-b662c9d.svg?x=7.7301%25&y=196

    `Flame graph`__ for psycopg3 at commit ``b662c9d``. Come and click around:
    it's interactive! 🍬

    .. __: http://www.brendangregg.com/flamegraphs.html

.. figure:: flame-psycopg2-2.8.5.png
    :align: center
    :target: flame-psycopg2-2.8.5.svg?x=12.2885%25&y=196

    Flame graph for psycopg2 2.8.5.

..
    for d in aiopg-tuples psycopg3-async; do
        py-spy record -r 500 -o $d.svg -F --native -- python pgbench_python.py $d \
            -C1 ../queries/1-pg_type.json -D 10 --warmup-time 1 \
            --pghost localhost --pgport 5432 --pguser piro --output-format json;
    done

The comparison shows that the time around the libpq is roughly comparable
(psycopg2 calls the chubby ``pqParseInput3`` as a consequence of calling
``PQnotifies``, for psycopg3 it happens in ``PQisBusy`` because it lacks a
notifications handler yet). There was actually `a bug`__ in the querying
process, but because of that bug the code was actually running somewhat faster
(it would have likely hurt concurrent performance by blocking though).

.. __: https://github.com/psycopg/psycopg3/commit/f9bd81d7e61628f619f39d399532fc4dedfab256


Current state, future improvements
==================================

The diagrams above show that there is still space for improvements:
``fetchall()`` seems to have some overhead which might be reduced by
introducing a "fetchsome" function in C. There is a lot of overhead around
waiting, especially in the process of registering/unregistering listeners: the
best gains can be probably obtained by creating a wait loop tightly integrated
with the current reactor bypassing some of the higher ``asyncio``
abstractions. Any takers?

These are the numbers, as of the current ``master`` (``b662c9d``). The Python
implementation has also improved somewhat thanks to some tweaking happened
since the first test):

===================== =======
Driver                Queries
===================== =======
asyncpg                 15263
psycopg2 + aiopg         8871
psycopg3 async Python     517
psycopg3 async C         8424
===================== =======

Again, this is just as a rough indication: the type of data types in the query
and different concurrency patterns create different pictures; for instance
psycopg3 is already faster than psycopg2 in benchmarks involving ``bytea``
thanks to the use of binary protocol:

================ =======
Driver           Queries
================ =======
asyncpg            41330
psycopg2 + aiopg   12396
psycopg3 async C   19164
================ =======

I think for the moment the search for speed can be concluded and it is time to
go back progressing with the features. I doubt that asyncpg's stellar
performances can be passed, but I don't want performance to be the sole goal:
I am fine with psycopg3 being somewhat slower than asyncpg if it has the
advantage to be easier to use and to behave pretty much as a drop-in
replacement for psycopg2. In my view a driver with a lower adoption barrier
would be more directly beneficial for everyone using it, either in existing
frameworks or on its own. By the time the project will be released I'm
confident that its performance will consistently pass the psycopg2 level.
